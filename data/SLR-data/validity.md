# Threats to Validity

To ensure the methodological rigor of our study and enhance transparency and reliability of our findings we adhered to the guidelines proposed by Ampatzoglou et al. [^1] and Wohlin et al. [^2] to relate our Threats to Validity.

## Internal

In our study, we opted to use only the SCOPUS database for selecting studies, rather than combining results from multiple sources. This decision was based on SCOPUS's extensive coverage of relevant journals and conferences [^5], as well as its status as one of the leading indexing databases in the scientific literature [^3]. We acknowledge that relying on a single database may introduce selection bias, since some pertinent studies might be indexed exclusively in other platforms. To mitigate this potential bias, we implemented a rigorous process for defining inclusion and exclusion criteria, ensuring transparency and reproducibility throughout the study selection. Moreover, previous research [^3] [^4] [^5] has similarly relied solely on SCOPUS as their search source. Nonetheless, we recommend that future research expand the search to include other databases.

Another threat related to study selection is the absence of manual search techniques, such as snowballing (searching for cited references or studies that cite the selected papers). We chose not to apply this approach because it could introduce older studies, diverting the focus of our review. Our goal is to analyze the current status of TTVs in replication studies, emphasizing the most recent research. To reinforce this strategy, we also restricted our search to the past three years, ensuring that the analyzed data reflect current trends and challenges in the replication literature. This limitation allows us to maintain an updated perspective, avoiding the inclusion of studies that may not represent the most recent scenario.

A significant validity threat in review studies is systematic error in data extraction, which occurs when there are inconsistencies or biases in the manual data collection process from the included studies. This issue can compromise the accuracy and reliability of the results. Such errors may arise due to various factors, including subjective interpretations, lack of standardization, and confirmation bias among researchers. To mitigate this threat, we applied a detailed data extraction protocol with clear and objective criteria. Data extraction was performed independently by two researchers, with discussions on any discrepancies. We also used standardized forms for data collection and ensured that both researchers involved in the data extraction process understood and applied the same established guidelines.

Another internal threat arises from how studies describe TTVs inconsistently. For example, while some articles provide detailed explanations of potential methodological weaknesses, others mention these threats only superficially or even omit them entirely. This bias can impact the comparative analysis of results, compromising the interpretation of findings. To mitigate this threat, in addition to a quantitative analysis of the categorization of reported TTVs, we considered only studies with a dedicated TTV section for our qualitative analysis.


## External

A relevant threat to the external validity of the study is the low methodological diversity. Most studies adopted Experimental Methods and Surveys making the conclusions potentially inapplicable to other methods like Case Studies, Action Research, and Ethnography. This limitation can affect the understanding about different methods. To mitigate this threat, we categorized the studies according to the research method used, enabling a differentiated analysis of the findings based on the adopted approach. This prevents improper extrapolation of conclusions to underrepresented methods. This limitation suggests the need for future investigations using diverse methods to validate and expand the applicability of the results.

To limit the search results to the Software Engineering field, we included the term "Software Engineering" in our search string using a conditional AND. This approach restricted the search to papers explicitly containing this term. While this practice may be common in search strategies focused on Software Engineering literature, it introduces a validity threat since relevant papers in the field may not explicitly mention "Software Engineering" in their text. Future research should consider expanding the search string with additional relevant keywords to achieve broader and more comprehensive coverage of studies published within the Software Engineering domain.


## Construct

A threat to construct validity in this review is the lack of clarity in defining the type of replication and research methods used in the analyzed studies. Many studies do not specify whether the replication performed is internal, external, close, differentiated, or conceptual, and some do not clearly indicate which methodological approach was applied (e.g., controlled experiments, case studies, surveys, ethnographies, or action research). This ambiguity complicates the accurate categorization of studies and may compromise the validity of our conclusions. The absence of detailed information can lead to inconsistencies in the analysis, as differing interpretations of the replication type and methodology may result in divergent classifications.

To mitigate this threat, for categorizing replicated studies we followed the guidelines proposed by Baldassarre et al. [^7]. For categorizing research methods, we followed Easterbrook et al. [^8]. These strategies reduce subjectivity in the analysis and enhance the reliability of our findings.

Another potential threat arises from the subjective nature of data categorization by researchers. Human judgment in classifying study characteristics can introduce bias or inconsistencies, potentially distorting the constructs' intended meaning. Systematic misclassification could also affect internal validity and weaken the strength of the conclusions. To minimize this risk, we employed careful calibration procedures and clear coding guidelines. Specifically, we relied on Baldassarre et al. [^7] for replication type classification and Easterbrook et al. [^8] for empirical method categorization. Furthermore, disagreements were resolved through discussion to ensure consistent interpretation across reviewers.



## Conclusion

The statistical alignment analysis between original and replication studies is subject to several threats to conclusion validity. One limitation is the relatively small number of replication studies available, which may restrict the statistical power and generalizability of the results. Additionally, low frequencies in some contingency table cells can compromise the robustness of chi-squared test outcomes, potentially affecting the reliability of the statistical inferences.

The binary classification of the presence or absence of threats to validity simplifies complex reporting practices and may fail to capture important nuances in the depth and quality of TTVs discussions. This reductionist approach can obscure meaningful variations across studies and should be taken into account when interpreting the reported associations.

Another threat to conclusion validity concerns the reliability of qualitative data analysis. Given that categorization depends on human judgment, there is an inherent risk of subjectivity and bias in coding decisions. Different researchers may interpret similar content in divergent ways, leading to inconsistencies and hampering comparability across studies. To mitigate this threat, we adopted established classification schemes, those proposed by Baldassarre et al. [^7] for replication types and by Easterbrook et al. [^8] for empirical research methods. Additionally, we incorporated a peer review process, whereby independent reviewers discussed and disagreements were resolved through discussion to ensure consistent interpretation across reviewers.

Finally, a potential threat to the comprehensiveness of this systematic review stems from the exclusive use of the SCOPUS database. Relevant studies not indexed in SCOPUS may have been inadvertently omitted, limiting the breadth and representativeness of the review. As noted by Silva et al. [^9], the scope of the literature search constitutes an inherent validity concern in systematic reviews. Achieving complete coverage is virtually impossible, and accurately estimating the degree of coverage remains a major challenge. Despite the adoption of a rigorous search strategy, the possibility of missing pertinent studies cannot be entirely eliminated.


# References

[^1]: Apostolos Ampatzoglou, Stamatia Bibi, Paris Avgeriou, and Alexander Chatzigeorgiou. 2020. Guidelines for Managing Threats to Validity of Secondary Studies in Software Engineering. Springer International Publishing, Cham, 415–441. doi:10.1007/978-3-030-32489-6_15.

[^2]: Claes Wohlin, Per Runeson, Martin Höst, Magnus C. Ohlsson, Böjrn Regnell, and Anders Wesslén. 2000. Experimentation in Software Engineering: An Introduction. Springer New York, NY. doi:10.1007/978-1-4615-4625-2.

[^3]: Edison Espinosa, Juan Ferreira, and Henry Chanatasig. 2018. Using Experimental Material Management Tools in Experimental Replication: A Systematic Mapping Study. 252–263. doi:10.1007/978-3-319-73450-7_25

[^4]: Margarita Cruz, Beatriz Bernárdez, Amador Durán, José A. Galindo, and Antonio Ruiz-Cortés. 2020. Replication of Studies in Empirical Software Engineering: A Systematic Mapping Study, From 2013 to 2018. IEEE Access 8 (2020), 26773–26791. doi:10.1109/ACCESS.2019.2952191.

[^5]: Andrea Fasciglione, Maurizio Leotta, and Alessandro Verri. 2022. Reproducibility in Activity Recognition Based on Wearable Devices: a Focus on Used Datasets. In 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 3178–3185. doi:10.1109/SMC53654.2022.9945344

[^6]: Organisation for Economic Co-operation and Development. 2015. Frascati Manual 2015: Guidelines for Collecting and Reporting Data on Research and Experimental Development. OECD. https://books.google.com.br/books?id=0jPojgEACAAJ

[^7]: Maria Teresa Baldassarre, Jeffrey Carver, Oscar Dieste, and Natalia Juristo. 2014. Replication types: towards a shared taxonomy. In Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering (London, England, United Kingdom) (EASE ’14). Association for Computing Machinery, New York, NY, USA, Article 18, 4 pages. doi:10.1145/2601248.2601299

[^8]: Steve Easterbrook, Janice Singer, Margaret-Anne Storey, and Daniela Damian. 2008. Selecting Empirical Methods for Software Engineering Research. Springer London, London, 285–311. doi:10.1007/978-1-84800-044-5_11.

[^9]: Fabio Q. Silva, Marcos Suassuna, A. César França, Alicia M. Grubb, Tatiana B. Gouveia, Cleviton V. Monteiro, and Igor Ebrahim Santos. 2014. Replication of empirical studies in software engineering research: a systematic mapping study. Empirical Softw. Engg. 19, 3 (June 2014), 501–557. doi:10.1007/s10664-012-9227-7.
